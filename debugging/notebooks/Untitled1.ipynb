{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "official-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "PAIR_FILE = \"../../data/review_rebuttal_pair_dataset/traindev_train.json\"\n",
    "\n",
    "with open(PAIR_FILE, 'r') as f:\n",
    "    pair_obj = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hidden-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pair_obj[\"review_rebuttal_pairs\"][0][\"review_text\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "traditional-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import scispacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "sci_nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "spacy_nlp = English()\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def question_tokenize(text):\n",
    "    inner_spans = []\n",
    "    parts = re.split(\"\\?\\s*\", text)\n",
    "    print(\"999\", parts)\n",
    "    for part in parts[:-1]:\n",
    "        part_start = text.find(part)\n",
    "        part_end = part_start + len(part) + 1\n",
    "        inner_spans.append((part_start, part_end))\n",
    "    inner_spans.append((text.find(parts[-1]), len(text)))\n",
    "    return inner_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excessive-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/// To my knowledge, this paper is probably the first one to apply few-shot learning concept into high-level computer vision tasks.\n",
      "/// In this paper's sense, segmentation.\n",
      "/// It proposes a general framework to few from the very few sample, extract a latent representation z, and apply it to do segmentation on a query.\n",
      "/// Cases of semantic, interactive and video segmentation are applied.\n",
      "/// Experiments are very thorough.\n",
      "/// \n",
      "\n",
      "We see too many variants of few-shot learning papers on mini-imagenet or omniglot.\n",
      "/// For the reason of applying to high-level segmentation, the paper already deserves an acceptance for the first work.\n",
      "/// I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\n",
      "\n",
      "Comments:\n",
      "\n",
      "- what is interactive segmentation? I looked through the related work, it just mentioned some previous work without defining or describing it.\n",
      "999 ['I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\\n\\nComments:\\n\\n- what is interactive segmentation', 'I looked through the related work, it just mentioned some previous work without defining or describing it.']\n",
      "/// \n",
      "\n",
      "\n",
      "/// - z is the network output of g? is there any constraint on z? Like Gaussian distributions like what z is like in VAE models.\n",
      "999 ['- z is the network output of g', 'is there any constraint on z', 'Like Gaussian distributions like what z is like in VAE models.']\n",
      "/// \n",
      "\n",
      "\n",
      ".\n",
      "To my knowledge, this paper is probably the first one to apply few-shot learning concept into high-level computer vision tasks.\n",
      ".\n",
      "In this paper's sense, segmentation.\n",
      ".\n",
      "It proposes a general framework to few from the very few sample, extract a latent representation z, and apply it to do segmentation on a query.\n",
      ".\n",
      "Cases of semantic, interactive and video segmentation are applied.\n",
      ".\n",
      "Experiments are very thorough.\n",
      ".\n",
      "\n",
      "\n",
      "We see too many variants of few-shot learning papers on mini-imagenet or omniglot.\n",
      ".\n",
      "For the reason of applying to high-level segmentation, the paper already deserves an acceptance for the first work.\n",
      ".\n",
      "I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\n",
      "\n",
      "Comments:\n",
      "\n",
      "- what is interactive segmentation?\n",
      ".\n",
      "I looked through the related work, it just mentioned some previous work without defining or describing it.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "- z is the network output of g?\n",
      ".\n",
      "is there any constraint on z?\n",
      ".\n",
      "Like Gaussian distributions like what z is like in VAE models.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "/// Summary:\n",
      "This paper proposed a few-shot learning approach for interactive segmentation.\n",
      "/// Given a set of user-annotated points, the proposed model learns to generate dense segmentation masks of objects.\n",
      "/// To incorporate the point-wise annotation, the guidance network is introduced.\n",
      "/// The proposed idea is applied to guided image segmentation, semantic segmentation, and video segmentation.\n",
      "/// \n",
      "\n",
      "Clarity:\n",
      "Overall, the presentation of the paper can be significantly improved.\n",
      "/// First of all, it is not clear what the problem setting of this paper is, as it seems to have two sets of training data of fully-annotated images (for training) and the combined set of point-wise annotated images and unannotated images (guidance images T in the first equation); It is not clear whether authors generate the second dataset out of the first one, or they have separate datasets for these two.\n",
      "/// Also, it is not clear how the authors incorporate the unannotated images for training.\n",
      "/// \n",
      "\n",
      "The descriptions on model architecture are also not quite clear, as it involves two components (g and f) but start discussing with g without providing a clear overview of the combined model (I would suggest changing the order of Section 4.1 and Section 4.2 to make it clearer).\n",
      "/// The loss functions are introduced in the last part of the method, which makes it also very difficult to understand.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Originality and significance:\n",
      "The technical contribution of the paper is very limited.\n",
      "/// I do not see many novel contributions in terms of both network architecture and learning perspective.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Experiment:\n",
      "Overall, I am not quite convinced with the experiment results.\n",
      "/// The method is compared against only a few (not popular) interactive segmentation methods, although there exist many recent works addressing the same task (e.g. Xu et al. 2016).\n",
      "/// \n",
      "\n",
      "The experiment settings are also not clearly presented.\n",
      "/// For instance, what is the dataset used for the evaluation of the first paragraph in section 5.1?\n",
      "/// How do you split the Pascal VOC data to exclusive sets? How do you sample point-wise annotation from dense mask labels? How does the sampling procedure affect the performance?\n",
      "999 ['How do you split the Pascal VOC data to exclusive sets', 'How do you sample point-wise annotation from dense mask labels', 'How does the sampling procedure affect the performance', '']\n",
      "/// \n",
      "\n",
      "The performance of the guided semantic segmentation is also quite low, limiting the practical usefulness of the method.\n",
      "/// Finally, the paper does not present qualitative results, which are essential to understanding the performance of the segmentation system.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Minor comments:\n",
      "1. There are a lot of grammar issues.\n",
      "/// Please revise your draft.\n",
      "/// \n",
      "\n",
      "/// 2. Please revise the notations in equations.\n",
      "/// For instance, \n",
      "    T = {{(x_1, L_1),...} \\cup {\\bar{x}_1,...}\n",
      "    L_s = {(p_j,l_j):j\\in{1,...,P}, l\\in{1,...,K}\\cup{\\emptyset}}\n",
      "    Also, in the next equation, j\\in\\bar{x}_q} -> p_ j\\in\\bar{x}_q} (j is an index of pixel)\n",
      "\n",
      ".\n",
      "Summary:\n",
      "This paper proposed a few-shot learning approach for interactive segmentation.\n",
      ".\n",
      "Given a set of user-annotated points, the proposed model learns to generate dense segmentation masks of objects.\n",
      ".\n",
      "To incorporate the point-wise annotation, the guidance network is introduced.\n",
      ".\n",
      "The proposed idea is applied to guided image segmentation, semantic segmentation, and video segmentation.\n",
      ".\n",
      "\n",
      "\n",
      "Clarity:\n",
      "Overall, the presentation of the paper can be significantly improved.\n",
      ".\n",
      "First of all, it is not clear what the problem setting of this paper is, as it seems to have two sets of training data of fully-annotated images (for training) and the combined set of point-wise annotated images and unannotated images (guidance images T in the first equation); It is not clear whether authors generate the second dataset out of the first one, or they have separate datasets for these two.\n",
      ".\n",
      "Also, it is not clear how the authors incorporate the unannotated images for training.\n",
      ".\n",
      "\n",
      "\n",
      "The descriptions on model architecture are also not quite clear, as it involves two components (g and f) but start discussing with g without providing a clear overview of the combined model (I would suggest changing the order of Section 4.1 and Section 4.2 to make it clearer).\n",
      ".\n",
      "The loss functions are introduced in the last part of the method, which makes it also very difficult to understand.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Originality and significance:\n",
      "The technical contribution of the paper is very limited.\n",
      ".\n",
      "I do not see many novel contributions in terms of both network architecture and learning perspective.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Experiment:\n",
      "Overall, I am not quite convinced with the experiment results.\n",
      ".\n",
      "The method is compared against only a few (not popular) interactive segmentation methods, although there exist many recent works addressing the same task (e.g. Xu et al. 2016).\n",
      ".\n",
      "\n",
      "\n",
      "The experiment settings are also not clearly presented.\n",
      ".\n",
      "For instance, what is the dataset used for the evaluation of the first paragraph in section 5.1?\n",
      ".\n",
      "How do you split the Pascal VOC data to exclusive sets?\n",
      ".\n",
      "How do you sample point-wise annotation from dense mask labels?\n",
      ".\n",
      "How does the sampling procedure affect the performance?\n",
      ".\n",
      "How do you split the Pascal VOC data to exclusive sets? How do you sample point-wise annotation from dense mask labels? How does the sampling procedure affect the performance?\n",
      ".\n",
      "\n",
      "\n",
      "The performance of the guided semantic segmentation is also quite low, limiting the practical usefulness of the method.\n",
      ".\n",
      "Finally, the paper does not present qualitative results, which are essential to understanding the performance of the segmentation system.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Minor comments:\n",
      "1. There are a lot of grammar issues.\n",
      ".\n",
      "Please revise your draft.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "2. Please revise the notations in equations.\n",
      ".\n",
      "For instance, \n",
      "    T = {{(x_1, L_1),...} \\cup {\\bar{x}_1,...}\n",
      "    L_s = {(p_j,l_j):j\\in{1,...,P}, l\\in{1,...,K}\\cup{\\emptyset}}\n",
      "    Also, in the next equation, j\\in\\bar{x}_q} -> p_ j\\in\\bar{x}_q} (j is an index of pixel)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/// Summary\n",
      "This paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.\n",
      "/// \n",
      "The main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.\n",
      "/// \n",
      "Specifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.\n",
      "/// \n",
      "By performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Strength\n",
      "Learning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.\n",
      "/// \n",
      "This paper tackles this problem and showed results on various segmentation problems.\n",
      "/// \n",
      "\n",
      "Weakness\n",
      "The proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach.\n",
      "/// Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging.\n",
      "/// These differences are relatively minor, so I question the novelty of this paper.\n",
      "/// \n",
      "\n",
      "This paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.\n",
      "/// \n",
      "For example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset.\n",
      "/// \n",
      "In addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.\n",
      "/// \n",
      "For example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.\n",
      "/// \n",
      "\n",
      "There are some strong arguments that require further justification.\n",
      "/// \n",
      "\n",
      "/// - In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).\n",
      "/// \n",
      "\n",
      "/// However, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem.\n",
      "/// The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.\n",
      "/// \n",
      "\n",
      "/// - In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation.\n",
      "/// I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes.\n",
      "/// So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Overall comment\n",
      "I believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method.\n",
      "/// \n",
      "\n",
      "/// Especially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.\n",
      "/// \n",
      "\n",
      ".\n",
      "Summary\n",
      "This paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.\n",
      ".\n",
      "\n",
      "The main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.\n",
      ".\n",
      "\n",
      "Specifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.\n",
      ".\n",
      "\n",
      "By performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Strength\n",
      "Learning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.\n",
      ".\n",
      "\n",
      "This paper tackles this problem and showed results on various segmentation problems.\n",
      ".\n",
      "\n",
      "\n",
      "Weakness\n",
      "The proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach.\n",
      ".\n",
      "Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging.\n",
      ".\n",
      "These differences are relatively minor, so I question the novelty of this paper.\n",
      ".\n",
      "\n",
      "\n",
      "This paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.\n",
      ".\n",
      "\n",
      "For example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset.\n",
      ".\n",
      "\n",
      "In addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.\n",
      ".\n",
      "\n",
      "For example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.\n",
      ".\n",
      "\n",
      "\n",
      "There are some strong arguments that require further justification.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "- In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "However, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem.\n",
      ".\n",
      "The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation.\n",
      ".\n",
      "I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes.\n",
      ".\n",
      "So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Overall comment\n",
      "I believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "Especially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.\n",
      ".\n",
      "\n",
      "\n",
      "/// The paper propose an end-to-end technique that applies both spatial and temporal attention.\n",
      "/// The spatial attention is done by training a mask-filter, while the temporal-attention use a soft-attention mechanism.\n",
      "///  In addition the authors propose several regularization terms  to directly improve attention.\n",
      "/// The evaluated datasets are action recognition datasets, such as HMDB51, UCF10, Moments in Time, THUMOS’14.\n",
      "/// The paper reports SOTA on all three datasets.\n",
      "/// \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/// Strengths:\n",
      "\n",
      "The paper is well written: easy to follow, and describe the importance of spatial-temporal attention.\n",
      "/// \n",
      "\n",
      "The model is simple, and propose novel attention regularization terms.\n",
      "/// \n",
      "\n",
      "The authors evaluates on several tasks, and shows good qualitative behavior.\n",
      "/// \n",
      "\n",
      "\n",
      "\n",
      "/// Weaknesses:\n",
      "\n",
      "The reported number on UCF101 and HMDB51 are confusing/misleading.\n",
      "///  Even with only RGB, the evaluation miss numbers of models like ActionVLAD with 50% on HMDB51 or Res3D with 88% on UCF101.\n",
      "/// I’ll also add that there are available models nowadays that achieve over 94% accuracy on UCF101, and over 72% on  HMDB51.\n",
      "/// The paper should at least have better discussion on those years of progress.\n",
      "/// The mis-information also continues in THUMOS14, for instance R-C3D beats the proposed model.\n",
      "/// \n",
      "\n",
      "In my opinion the paper should include a flow variant.\n",
      "/// It is a common setup in action recognition, and a good model should take advantage of these features.\n",
      "/// Especially for spatial-temporal attention, e.g., VideoLSTM paper by Li\n",
      "/// .\n",
      "/// \n",
      "\n",
      "In general spatial attention over each frame is extremely demanding.\n",
      "/// The original image features are now multiplied by 49 factor, this is more demanding in terms of memory consumption than the flow features they chose to ignore.\n",
      "///  The authors reports on 15-frames datasets for those short videos.\n",
      "/// But it will be interesting to see if the model is still useable on longer videos, for instance on Charades dataset.\n",
      "/// \n",
      "\n",
      "Can you please explain why you chose a regularized making instead of Soft-attention for spatial attention?\n",
      "/// \n",
      "\n",
      "\n",
      "/// To conclude: \n",
      "The goal of spatial-temporal attention is important, and the proposed approach behaves well.\n",
      "/// Yet the model is an extension of known techniques for image attention, which are not trivial to apply on long-videos with many frames.\n",
      "/// Evaluating only on rgb features is not enough for an action recognition model.\n",
      "/// Importantly, even when considering only rgb models, the paper still missed many popular stronger baselines.\n",
      "/// \n",
      "\n",
      "\n",
      ".\n",
      "The paper propose an end-to-end technique that applies both spatial and temporal attention.\n",
      ".\n",
      "The spatial attention is done by training a mask-filter, while the temporal-attention use a soft-attention mechanism.\n",
      ".\n",
      " In addition the authors propose several regularization terms  to directly improve attention.\n",
      ".\n",
      "The evaluated datasets are action recognition datasets, such as HMDB51, UCF10, Moments in Time, THUMOS’14.\n",
      ".\n",
      "The paper reports SOTA on all three datasets.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Strengths:\n",
      "\n",
      "The paper is well written: easy to follow, and describe the importance of spatial-temporal attention.\n",
      ".\n",
      "\n",
      "\n",
      "The model is simple, and propose novel attention regularization terms.\n",
      ".\n",
      "\n",
      "\n",
      "The authors evaluates on several tasks, and shows good qualitative behavior.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Weaknesses:\n",
      "\n",
      "The reported number on UCF101 and HMDB51 are confusing/misleading.\n",
      ".\n",
      " Even with only RGB, the evaluation miss numbers of models like ActionVLAD with 50% on HMDB51 or Res3D with 88% on UCF101.\n",
      ".\n",
      "I’ll also add that there are available models nowadays that achieve over 94% accuracy on UCF101, and over 72% on  HMDB51.\n",
      ".\n",
      "The paper should at least have better discussion on those years of progress.\n",
      ".\n",
      "The mis-information also continues in THUMOS14, for instance R-C3D beats the proposed model.\n",
      ".\n",
      "\n",
      "\n",
      "In my opinion the paper should include a flow variant.\n",
      ".\n",
      "It is a common setup in action recognition, and a good model should take advantage of these features.\n",
      ".\n",
      "Especially for spatial-temporal attention, e.g., VideoLSTM paper by Li\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "\n",
      "In general spatial attention over each frame is extremely demanding.\n",
      ".\n",
      "The original image features are now multiplied by 49 factor, this is more demanding in terms of memory consumption than the flow features they chose to ignore.\n",
      ".\n",
      " The authors reports on 15-frames datasets for those short videos.\n",
      ".\n",
      "But it will be interesting to see if the model is still useable on longer videos, for instance on Charades dataset.\n",
      ".\n",
      "\n",
      "\n",
      "Can you please explain why you chose a regularized making instead of Soft-attention for spatial attention?\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "To conclude: \n",
      "The goal of spatial-temporal attention is important, and the proposed approach behaves well.\n",
      ".\n",
      "Yet the model is an extension of known techniques for image attention, which are not trivial to apply on long-videos with many frames.\n",
      ".\n",
      "Evaluating only on rgb features is not enough for an action recognition model.\n",
      ".\n",
      "Importantly, even when considering only rgb models, the paper still missed many popular stronger baselines.\n",
      ".\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/// # 1\n",
      "/// .\n",
      "/// Summary\n",
      "This paper presents a novel spatio-temporal attention mechanism.\n",
      "/// The spatial attention is decomposed from the temporal attention and acts on each frame independently, while the temporal attention is applied on top of it on the temporal domain.\n",
      "/// The main contribution of the paper is the introduction of regularisers that improve performance and interpretability of the model.\n",
      "/// \n",
      "\n",
      "Strengths:\n",
      "* Quality of the paper, although some points need to clarified and expanded a bit more (see #2)\n",
      "* Nice diversity of experiments, datasets and tasks that the method is tested on (see #4)\n",
      "/// \n",
      "\n",
      "\n",
      "/// Weaknesses:\n",
      "* The paper do not present substantial novelty compared to previous work (see #3)\n",
      "\n",
      "\n",
      "# 2.\n",
      "/// Clarity and Motivation\n",
      "The paper is in general clear and well motivated, however there are few points that need to be improved:\n",
      "* How is the importance mask (Eq. 1) is defined?\n",
      "/// The authors said “we simply use three convolutional layers to learn the importance mask”, however the convolutional output should be somehow processed to get out the importance map, in order to match the same sizes of X_i.\n",
      "/// The details of this network are missing to be able to reproduce the model.\n",
      "/// \n",
      "\n",
      "/// * The authors introduced \\phi(H) and \\phi(X) which are feedforward networks, but their definition and specifics are not mentioned in the paper.\n",
      "/// \n",
      "* It is not clear how Eq. 9 performs regularization of the mask.\n",
      "/// Can the authors give an intuition about the definition of L_{contrast}? What does it encourages? In which cases might it be useful?\n",
      "999 ['Can the authors give an intuition about the definition of L_{contrast}', 'What does it encourages', 'In which cases might it be useful', '']\n",
      "/// \n",
      "* Why does L_{unimodal} need to encourage the temporal attention weights to be unimodal?\n",
      "/// It seems that the assumption is valid because of the nature of the dataset, i.e., the video clips contain only a single action with some “background” frames in the beginning and the end.\n",
      "/// This is not valid in general. Can the authors discuss about this maybe with an example?\n",
      "/// \n",
      "\n",
      "\n",
      "\n",
      "/// #\n",
      "/// 3. Novelty\n",
      "The main concern of the proposal in this paper is its novelty.\n",
      "/// Temporal attention pooling have been explored in other papers; just to cite a popular one among others:\n",
      "/// \n",
      "\n",
      "/// * Long, Xiang, et al.\n",
      "/// \"Attention clusters: Purely attention based local feature integration for video classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n",
      "/// 2018\n",
      "/// .\n",
      "/// \n",
      "* Other paper from the youtube8m workshops explore the same ideas: https://research.google.com/youtube8m/workshop2017/ \n",
      "Sec. 2.2 should be expanded by including papers and discuss how the presented temporal attention differs from that.\n",
      "/// \n",
      "\n",
      "Moreover spatio-temporal attention has been previously explored.\n",
      "/// For example, the following paper also decouple the spatial and temporal component as the proposal:\n",
      "/// \n",
      "* Song, Sijie, et al. \"An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data.\" AAAI. Vol. 1. No. 2. 2017.\n",
      "/// \n",
      "This is just an example, but there are there are other papers that model the spatio-temporal extent of videos without attention for action recognition.\n",
      "/// The authors should expand Sec. 2 by including such relevant literature.\n",
      "/// \n",
      "\n",
      "\n",
      "\n",
      "/// #\n",
      "/// 4. Experimentation\n",
      "The experiments are carried on video action recognition task on three public available datasets, including HMDB51, UCF101 and Moments in Time.\n",
      "/// The authors show a nice ablation study by removing the main components of the proposed method and show nice improvements with respect to some baseline\n",
      "/// (Table 1).\n",
      "/// Although the results are not too close to the state of the art for video action recognition on HMDB51 and UCF101, the authors first show nice accuracy on Moments in Time (Table 2).\n",
      "/// \n",
      "\n",
      "\n",
      "/// Moreover the authors show that the model can be useful on the more challenging task of weakly supervised action localization (UCF101-24, THUMOS).\n",
      "/// Specifically, spatial attention is used to localize the action in each frame by thresholding, showing competitive results\n",
      "/// (Table 3)\n",
      "/// .\n",
      "/// Although some more recent references are missing, see the following paper for example:\n",
      "* G. Singh, S Saha, M. Sapienza, P. H. S. Torr and F Cuzzolin.\n",
      "/// \"Online Real time Multiple Spatiotemporal Action Localisation and Prediction.\" ICCV, 2017.\n",
      "/// \n",
      "\n",
      "/// Then the authors tested also for temporal action localization (Table 4).\n",
      "/// \n",
      "\n",
      "In general, the paper is not showing state-of-the-art results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.\n",
      "/// \n",
      "\n",
      "\n",
      ".\n",
      "# 1\n",
      ".\n",
      ".\n",
      ".\n",
      "Summary\n",
      "This paper presents a novel spatio-temporal attention mechanism.\n",
      ".\n",
      "The spatial attention is decomposed from the temporal attention and acts on each frame independently, while the temporal attention is applied on top of it on the temporal domain.\n",
      ".\n",
      "The main contribution of the paper is the introduction of regularisers that improve performance and interpretability of the model.\n",
      ".\n",
      "\n",
      "\n",
      "Strengths:\n",
      "* Quality of the paper, although some points need to clarified and expanded a bit more (see #2)\n",
      "* Nice diversity of experiments, datasets and tasks that the method is tested on (see #4)\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Weaknesses:\n",
      "* The paper do not present substantial novelty compared to previous work (see #3)\n",
      "\n",
      "\n",
      "# 2.\n",
      ".\n",
      "Clarity and Motivation\n",
      "The paper is in general clear and well motivated, however there are few points that need to be improved:\n",
      "* How is the importance mask (Eq. 1) is defined?\n",
      ".\n",
      "The authors said “we simply use three convolutional layers to learn the importance mask”, however the convolutional output should be somehow processed to get out the importance map, in order to match the same sizes of X_i.\n",
      ".\n",
      "The details of this network are missing to be able to reproduce the model.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "* The authors introduced \\phi(H) and \\phi(X) which are feedforward networks, but their definition and specifics are not mentioned in the paper.\n",
      ".\n",
      "\n",
      "* It is not clear how Eq. 9 performs regularization of the mask.\n",
      ".\n",
      "Can the authors give an intuition about the definition of L_{contrast}?\n",
      ".\n",
      "What does it encourages?\n",
      ".\n",
      "In which cases might it be useful?\n",
      ".\n",
      "Can the authors give an intuition about the definition of L_{contrast}? What does it encourages? In which cases might it be useful?\n",
      ".\n",
      "\n",
      "* Why does L_{unimodal} need to encourage the temporal attention weights to be unimodal?\n",
      ".\n",
      "It seems that the assumption is valid because of the nature of the dataset, i.e., the video clips contain only a single action with some “background” frames in the beginning and the end.\n",
      ".\n",
      "This is not valid in general. Can the authors discuss about this maybe with an example?\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "#\n",
      ".\n",
      "3. Novelty\n",
      "The main concern of the proposal in this paper is its novelty.\n",
      ".\n",
      "Temporal attention pooling have been explored in other papers; just to cite a popular one among others:\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "* Long, Xiang, et al.\n",
      ".\n",
      "\"Attention clusters: Purely attention based local feature integration for video classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n",
      ".\n",
      "2018\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "* Other paper from the youtube8m workshops explore the same ideas: https://research.google.com/youtube8m/workshop2017/ \n",
      "Sec. 2.2 should be expanded by including papers and discuss how the presented temporal attention differs from that.\n",
      ".\n",
      "\n",
      "\n",
      "Moreover spatio-temporal attention has been previously explored.\n",
      ".\n",
      "For example, the following paper also decouple the spatial and temporal component as the proposal:\n",
      ".\n",
      "\n",
      "* Song, Sijie, et al. \"An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data.\" AAAI. Vol. 1. No. 2. 2017.\n",
      ".\n",
      "\n",
      "This is just an example, but there are there are other papers that model the spatio-temporal extent of videos without attention for action recognition.\n",
      ".\n",
      "The authors should expand Sec. 2 by including such relevant literature.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "#\n",
      ".\n",
      "4. Experimentation\n",
      "The experiments are carried on video action recognition task on three public available datasets, including HMDB51, UCF101 and Moments in Time.\n",
      ".\n",
      "The authors show a nice ablation study by removing the main components of the proposed method and show nice improvements with respect to some baseline\n",
      ".\n",
      "(Table 1).\n",
      ".\n",
      "Although the results are not too close to the state of the art for video action recognition on HMDB51 and UCF101, the authors first show nice accuracy on Moments in Time (Table 2).\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Moreover the authors show that the model can be useful on the more challenging task of weakly supervised action localization (UCF101-24, THUMOS).\n",
      ".\n",
      "Specifically, spatial attention is used to localize the action in each frame by thresholding, showing competitive results\n",
      ".\n",
      "(Table 3)\n",
      ".\n",
      ".\n",
      ".\n",
      "Although some more recent references are missing, see the following paper for example:\n",
      "* G. Singh, S Saha, M. Sapienza, P. H. S. Torr and F Cuzzolin.\n",
      ".\n",
      "\"Online Real time Multiple Spatiotemporal Action Localisation and Prediction.\" ICCV, 2017.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "Then the authors tested also for temporal action localization (Table 4).\n",
      ".\n",
      "\n",
      "\n",
      "In general, the paper is not showing state-of-the-art results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.\n",
      ".\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/// A method for activity recognition in videos is presented, which uses spatial soft attention combined with temporal soft attention.\n",
      "/// In a nutshell, a pixelwise mask is output and elementwise combined with feature maps for spatial attention, and temporal attention is a distribution over frames.\n",
      "/// The method is tested on several datasets.\n",
      "/// \n",
      "\n",
      "\n",
      "/// My biggest concern with the paper is novelty, which is rather low.\n",
      "/// Attention models are one of the most highly impactful discoveries in deep learning, which have been widely and extensively studied in computer vision, and also in activity recognition.\n",
      "/// Spatial and temporal attention mechanisms are now widely used by the community.\n",
      "/// I am not sure to see the exact novelty of the proposed, it seems to be very classic: soft attention over feature maps and frames is not new.\n",
      "/// Using attention distributions for localization has also been shown in the past.\n",
      "/// \n",
      "\n",
      "\n",
      "/// This also shows in the related works section, which contains only 3 references for spatial attention and only 2 references for temporal attention out of a vast body of known work.\n",
      "/// \n",
      "\n",
      "The unimodality prior (implemented as log concave prior) is interesting, but uni-modality is a very strong assumption.\n",
      "/// While it could be argued that spurior attention should be avoided, unimodality is much less clear.\n",
      "/// For this reason, the prior should be compared with even simpler priors, like total variation over time (similar to what has been done over space).\n",
      "/// \n",
      "\n",
      "\n",
      "/// The ablation study in the experimental section shows, that the different mechanisms only marginally contribute to the performance of the method: +0.7p on HMDB51, slightly more on UCF101.\n",
      "/// Similarly, the different loss functions only very marginally contribute to the performance.\n",
      "/// \n",
      "\n",
      "The method is only compared to Sharma 2015 on these datasets, which starts to be dated and is not state of the art anymore.\n",
      "/// Activity recognition has recently very much benefitted from optimization of convolutional backbones, like I3D and variants.\n",
      "/// \n",
      "\n",
      "The LSTM equations at the end of page are unnecessary because widely known.\n",
      "/// \n",
      "\n",
      ".\n",
      "A method for activity recognition in videos is presented, which uses spatial soft attention combined with temporal soft attention.\n",
      ".\n",
      "In a nutshell, a pixelwise mask is output and elementwise combined with feature maps for spatial attention, and temporal attention is a distribution over frames.\n",
      ".\n",
      "The method is tested on several datasets.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "My biggest concern with the paper is novelty, which is rather low.\n",
      ".\n",
      "Attention models are one of the most highly impactful discoveries in deep learning, which have been widely and extensively studied in computer vision, and also in activity recognition.\n",
      ".\n",
      "Spatial and temporal attention mechanisms are now widely used by the community.\n",
      ".\n",
      "I am not sure to see the exact novelty of the proposed, it seems to be very classic: soft attention over feature maps and frames is not new.\n",
      ".\n",
      "Using attention distributions for localization has also been shown in the past.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "This also shows in the related works section, which contains only 3 references for spatial attention and only 2 references for temporal attention out of a vast body of known work.\n",
      ".\n",
      "\n",
      "\n",
      "The unimodality prior (implemented as log concave prior) is interesting, but uni-modality is a very strong assumption.\n",
      ".\n",
      "While it could be argued that spurior attention should be avoided, unimodality is much less clear.\n",
      ".\n",
      "For this reason, the prior should be compared with even simpler priors, like total variation over time (similar to what has been done over space).\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "The ablation study in the experimental section shows, that the different mechanisms only marginally contribute to the performance of the method: +0.7p on HMDB51, slightly more on UCF101.\n",
      ".\n",
      "Similarly, the different loss functions only very marginally contribute to the performance.\n",
      ".\n",
      "\n",
      "\n",
      "The method is only compared to Sharma 2015 on these datasets, which starts to be dated and is not state of the art anymore.\n",
      ".\n",
      "Activity recognition has recently very much benefitted from optimization of convolutional backbones, like I3D and variants.\n",
      ".\n",
      "\n",
      "\n",
      "The LSTM equations at the end of page are unnecessary because widely known.\n",
      ".\n",
      "\n",
      "\n",
      "/// ---Below is based on the original paper---\n",
      "This paper presents a framework that allows the agent to learn from its observations, but never follows through on the motivation of experimentation---taking actions mainly for the purpose of learning an improved dynamics model.\n",
      "/// All of their experiments merely take actions that are best according to the usual model-based or model-free methods, and show that their consistency constraint allows them to learn a better dynamics model, which is not at all surprising.\n",
      "/// They do not even allow for the type of experimentation that has been done in reinforcement learning for as long as it has been around, which is to allow exploration by artificially increasing the reward for the first few times that each state is visited.\n",
      "/// That would be a good baseline against which to compare their method.\n",
      "/// \n",
      "\n",
      "Overall:\n",
      "Pros:\n",
      "1. Clear writing\n",
      "2. Good motivation description.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Cons:\n",
      "1. Failed to connect presented work with the motivation.\n",
      "/// \n",
      "2. No comparison against known methods for exploration.\n",
      "/// \n",
      "\n",
      "\n",
      "\n",
      "/// ----Below is based on the revision---\n",
      "\n",
      "Thanks to the reviewers for making the paper much clearer. I have no particular issues on the items that are in the paper. However, subsections 7.2.1 and 7.2.2 are missing.\n",
      ".\n",
      "---Below is based on the original paper---\n",
      "This paper presents a framework that allows the agent to learn from its observations, but never follows through on the motivation of experimentation---taking actions mainly for the purpose of learning an improved dynamics model.\n",
      ".\n",
      "All of their experiments merely take actions that are best according to the usual model-based or model-free methods, and show that their consistency constraint allows them to learn a better dynamics model, which is not at all surprising.\n",
      ".\n",
      "They do not even allow for the type of experimentation that has been done in reinforcement learning for as long as it has been around, which is to allow exploration by artificially increasing the reward for the first few times that each state is visited.\n",
      ".\n",
      "That would be a good baseline against which to compare their method.\n",
      ".\n",
      "\n",
      "\n",
      "Overall:\n",
      "Pros:\n",
      "1. Clear writing\n",
      "2. Good motivation description.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Cons:\n",
      "1. Failed to connect presented work with the motivation.\n",
      ".\n",
      "\n",
      "2. No comparison against known methods for exploration.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "----Below is based on the revision---\n",
      "\n",
      "Thanks to the reviewers for making the paper much clearer. I have no particular issues on the items that are in the paper. However, subsections 7.2.1 and 7.2.2 are missing.\n",
      "/// \n",
      "-------------\n",
      "Summary\n",
      "-------------\n",
      "The authors propose to train a policy while concurrently learning a dynamics model.\n",
      "/// In particular, the policy is updated using both the RL loss (rewards from the environment) and the \"consistency constraint\", which the authors introduce.\n",
      "/// This consistency constraint is a supervised learning signal, which compares trajectories in the environment with trajectories in the imagined world (produced with the dynamics model).\n",
      "/// \n",
      "\n",
      "---------------------\n",
      "Main Feedback\n",
      "---------------------\n",
      "I feel like there might be some interesting ideas in this work, and the results suggest that this approach performs well.\n",
      "/// However, I had a difficult time understanding how exactly the method works, and what its advantages are.\n",
      "/// These are my main questions:\n",
      "\n",
      "1) At the beginning of Section 4 the authors write \"The learning agent has two pathways for improving its behaviour: (...) (ii) the open loop path, where it imagines taking actions and hallucinates the state transitions that could happen\". Do you actually do this? This is not mentioned in anywhere. And as far as I understand, the reward function is not learned - hence there will be no training signal in the open loop path.\n",
      "999 ['These are my main questions:\\n\\n1) At the beginning of Section 4 the authors write \"The learning agent has two pathways for improving its behaviour: (...) (ii) the open loop path, where it imagines taking actions and hallucinates the state transitions that could happen\". Do you actually do this', 'This is not mentioned in anywhere. And as far as I understand, the reward function is not learned - hence there will be no training signal in the open loop path.']\n",
      "/// Does the reward signal always come from the true environment?\n",
      "/// \n",
      "2) Is the dynamics model used for anything else than action-selection during training?\n",
      "/// Planning?\n",
      "/// If not, I don't really understand the results and why this works at all (k=20 being better than k=5, for example).\n",
      "/// \n",
      "\n",
      "/// 3) Is the dynamics model pre-trained in any way? I find it surprising that the model-free method and the proposed method perform similar at the beginning (Figure 3).\n",
      "999 ['3) Is the dynamics model pre-trained in any way', 'I find it surprising that the model-free method and the proposed method perform similar at the beginning (Figure 3).']\n",
      "/// If the agent chooses its actions based on the state that is predicted by the dynamics model, this should throw off the learning of the policy at the beginning (when the dynamics model hasn't learned anything sensible yet).\n",
      "/// \n",
      "\n",
      "-----------------------\n",
      "Other Questions\n",
      "/// \n",
      "-----------------------\n",
      "4) How exactly does training without the consistency constraint look? Is this the same as k=1?\n",
      "999 ['\\n-----------------------\\n4) How exactly does training without the consistency constraint look', 'Is this the same as k=1', '']\n",
      "/// \n",
      "5) Could the authors comment on the evaluation protocol in the experimental section? Are the results averages over multiple runs? If so, it would help to see confidence intervals to make a fair assessment of the results.\n",
      "999 ['\\n5) Could the authors comment on the evaluation protocol in the experimental section', 'Are the results averages over multiple runs', 'If so, it would help to see confidence intervals to make a fair assessment of the results.']\n",
      "/// \n",
      "6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so? If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)?\n",
      "999 ['\\n6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so', \"If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)\", '']\n",
      "/// \n",
      "\n",
      "\n",
      "/// ---------\n",
      "\n",
      "/// Clarity\n",
      "/// \n",
      "---------\n",
      "/// \n",
      "The title and introduction initially gave me a slightly wrong impression on what the paper is going to be about, and several things were not followed up on later in the paper.\n",
      "/// \n",
      "\n",
      "/// Title:\n",
      "8) \"generative models\" reminds of things like a VAE or GAN; however, I believe the authors mean \"dynamics models\" instead\n",
      "9) \"by interaction\" is a bit vague as to what the contribution is (aren't policies and dynamic models in general trained by interacting with the environment?); the main idea of the paper is the consistency constraint\n",
      "Abstract / Introduction:\n",
      "10) The authors talk about humans carrying out \"experiments via interaction\" to help uncover \"true causal relationships\".\n",
      "999 ['Title:\\n8) \"generative models\" reminds of things like a VAE or GAN; however, I believe the authors mean \"dynamics models\" instead\\n9) \"by interaction\" is a bit vague as to what the contribution is (aren\\'t policies and dynamic models in general trained by interacting with the environment', '); the main idea of the paper is the consistency constraint\\nAbstract / Introduction:\\n10) The authors talk about humans carrying out \"experiments via interaction\" to help uncover \"true causal relationships\".']\n",
      "/// This idea is not brought up again in the methods section, and I don't see evidence that with the proposed approach, the policy does targeted experiments to uncover causal relationships.\n",
      "/// It is not clear to me why this is the intuition that motivates the consistency constraint.\n",
      "/// \n",
      "11) As the authors state in the introduction, the hope of model-based RL is better sample complexity.\n",
      "/// This is usually achieved by using the model in some way, for example by planning several steps ahead when choosing the current action.\n",
      "/// Could the authors comment on where they would place their proposed method - how does it address sample complexity?\n",
      "/// \n",
      "12) In the introduction, the authors discuss the problem of compounding errors.\n",
      "/// These must be a problem in the proposed method as well, especially as k grows.\n",
      "/// Could the authors comment on that? How come that the performance is so good for k=20?\n",
      "999 ['Could the authors comment on that', 'How come that the performance is so good for k=20', '']\n",
      "/// \n",
      "13) The authors write that in most model-based approaches, the dynamics model is \"learned with supervised learning techniques, i.e., just by observing the data\" and not via interaction.\n",
      "/// There's two things I don't understand: (1) in the existing model-based approaches the authors refer to, the policy also interacts with the world to get the data to do supervised learning - what exactly is the difference?\n",
      "/// (2) The auxiliary loss \"which explicitly seeks to match the generative behaviour to the observed behaviour\" is just a supervised learning loss as well, so how is this different?\n",
      "/// \n",
      "\n",
      "For me, it would help the readability and understanding of the paper if some concepts were introduced more formally.\n",
      "/// \n",
      "14) In Section 2, it would help me to see a formal definition of the MDP and what exactly is optimised.\n",
      "/// The authors write \"optimise a reward signal\" and \"maximise its expected reward\", however I believe it should be the expected cumulative reward (i.e., return).\n",
      "/// \n",
      "15) The loss function for the dynamics model is not explicitly stated.\n",
      "/// From the text I assume that it is the mean squared error for the per-step loss, and a GAN loss for the trajectory-wise loss.\n",
      "/// \n",
      "\n",
      "/// 16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined? Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss?\n",
      "999 ['16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined', 'Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss', '']\n",
      "/// \n",
      "17) In 2.3 the variable z_t is not formally introduced. What does it represent?\n",
      "/// \n",
      "\n",
      "------------------------\n",
      "Other Comments\n",
      "/// \n",
      "------------------------\n",
      "18) I find it problematic to use words such as \"hallucination\" and \"imagination\" when talking about learning algorithms.\n",
      "/// I would much prefer to see formal/factual language (like saying that the dynamics model is used to do make predictions / do planning, rather than that the agent is hallucinating).\n",
      "/// \n",
      "\n",
      "-- edit (19.11.) ---\n",
      "- updated score to 5\n",
      "- corrected summary\n",
      ".\n",
      "\n",
      "-------------\n",
      "Summary\n",
      "-------------\n",
      "The authors propose to train a policy while concurrently learning a dynamics model.\n",
      ".\n",
      "In particular, the policy is updated using both the RL loss (rewards from the environment) and the \"consistency constraint\", which the authors introduce.\n",
      ".\n",
      "This consistency constraint is a supervised learning signal, which compares trajectories in the environment with trajectories in the imagined world (produced with the dynamics model).\n",
      ".\n",
      "\n",
      "\n",
      "---------------------\n",
      "Main Feedback\n",
      "---------------------\n",
      "I feel like there might be some interesting ideas in this work, and the results suggest that this approach performs well.\n",
      ".\n",
      "However, I had a difficult time understanding how exactly the method works, and what its advantages are.\n",
      ".\n",
      "These are my main questions:\n",
      "\n",
      "1) At the beginning of Section 4 the authors write \"The learning agent has two pathways for improving its behaviour: (...) (ii) the open loop path, where it imagines taking actions and hallucinates the state transitions that could happen\". Do you actually do this?\n",
      ".\n",
      "This is not mentioned in anywhere. And as far as I understand, the reward function is not learned - hence there will be no training signal in the open loop path.\n",
      ".\n",
      "Does the reward signal always come from the true environment?\n",
      ".\n",
      "\n",
      "2) Is the dynamics model used for anything else than action-selection during training?\n",
      ".\n",
      "Planning?\n",
      ".\n",
      "If not, I don't really understand the results and why this works at all (k=20 being better than k=5, for example).\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "3) Is the dynamics model pre-trained in any way?\n",
      ".\n",
      "I find it surprising that the model-free method and the proposed method perform similar at the beginning (Figure 3).\n",
      ".\n",
      "If the agent chooses its actions based on the state that is predicted by the dynamics model, this should throw off the learning of the policy at the beginning (when the dynamics model hasn't learned anything sensible yet).\n",
      ".\n",
      "\n",
      "\n",
      "-----------------------\n",
      "Other Questions\n",
      ".\n",
      "\n",
      "-----------------------\n",
      "4) How exactly does training without the consistency constraint look?\n",
      ".\n",
      "Is this the same as k=1?\n",
      ".\n",
      "\n",
      "-----------------------\n",
      "4) How exactly does training without the consistency constraint look? Is this the same as k=1?\n",
      ".\n",
      "\n",
      "5) Could the authors comment on the evaluation protocol in the experimental section?\n",
      ".\n",
      "Are the results averages over multiple runs?\n",
      ".\n",
      "If so, it would help to see confidence intervals to make a fair assessment of the results.\n",
      ".\n",
      "\n",
      "6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so?\n",
      ".\n",
      "If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)?\n",
      ".\n",
      "\n",
      "6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so? If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)?\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "---------\n",
      "\n",
      ".\n",
      "Clarity\n",
      ".\n",
      "\n",
      "---------\n",
      ".\n",
      "\n",
      "The title and introduction initially gave me a slightly wrong impression on what the paper is going to be about, and several things were not followed up on later in the paper.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "Title:\n",
      "8) \"generative models\" reminds of things like a VAE or GAN; however, I believe the authors mean \"dynamics models\" instead\n",
      "9) \"by interaction\" is a bit vague as to what the contribution is (aren't policies and dynamic models in general trained by interacting with the environment?\n",
      ".\n",
      "); the main idea of the paper is the consistency constraint\n",
      "Abstract / Introduction:\n",
      "10) The authors talk about humans carrying out \"experiments via interaction\" to help uncover \"true causal relationships\".\n",
      ".\n",
      "This idea is not brought up again in the methods section, and I don't see evidence that with the proposed approach, the policy does targeted experiments to uncover causal relationships.\n",
      ".\n",
      "It is not clear to me why this is the intuition that motivates the consistency constraint.\n",
      ".\n",
      "\n",
      "11) As the authors state in the introduction, the hope of model-based RL is better sample complexity.\n",
      ".\n",
      "This is usually achieved by using the model in some way, for example by planning several steps ahead when choosing the current action.\n",
      ".\n",
      "Could the authors comment on where they would place their proposed method - how does it address sample complexity?\n",
      ".\n",
      "\n",
      "12) In the introduction, the authors discuss the problem of compounding errors.\n",
      ".\n",
      "These must be a problem in the proposed method as well, especially as k grows.\n",
      ".\n",
      "Could the authors comment on that?\n",
      ".\n",
      "How come that the performance is so good for k=20?\n",
      ".\n",
      "Could the authors comment on that? How come that the performance is so good for k=20?\n",
      ".\n",
      "\n",
      "13) The authors write that in most model-based approaches, the dynamics model is \"learned with supervised learning techniques, i.e., just by observing the data\" and not via interaction.\n",
      ".\n",
      "There's two things I don't understand: (1) in the existing model-based approaches the authors refer to, the policy also interacts with the world to get the data to do supervised learning - what exactly is the difference?\n",
      ".\n",
      "(2) The auxiliary loss \"which explicitly seeks to match the generative behaviour to the observed behaviour\" is just a supervised learning loss as well, so how is this different?\n",
      ".\n",
      "\n",
      "\n",
      "For me, it would help the readability and understanding of the paper if some concepts were introduced more formally.\n",
      ".\n",
      "\n",
      "14) In Section 2, it would help me to see a formal definition of the MDP and what exactly is optimised.\n",
      ".\n",
      "The authors write \"optimise a reward signal\" and \"maximise its expected reward\", however I believe it should be the expected cumulative reward (i.e., return).\n",
      ".\n",
      "\n",
      "15) The loss function for the dynamics model is not explicitly stated.\n",
      ".\n",
      "From the text I assume that it is the mean squared error for the per-step loss, and a GAN loss for the trajectory-wise loss.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined?\n",
      ".\n",
      "Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss?\n",
      ".\n",
      "16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined? Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss?\n",
      ".\n",
      "\n",
      "17) In 2.3 the variable z_t is not formally introduced. What does it represent?\n",
      ".\n",
      "\n",
      "\n",
      "------------------------\n",
      "Other Comments\n",
      ".\n",
      "\n",
      "------------------------\n",
      "18) I find it problematic to use words such as \"hallucination\" and \"imagination\" when talking about learning algorithms.\n",
      ".\n",
      "I would much prefer to see formal/factual language (like saying that the dynamics model is used to do make predictions / do planning, rather than that the agent is hallucinating).\n",
      ".\n",
      "\n",
      "\n",
      "-- edit (19.11.) ---\n",
      "- updated score to 5\n",
      "- corrected summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/// \n",
      "\n",
      "/// Summary:\n",
      "\n",
      "This paper presents a simple auxiliary loss term for model-based RL that attempts to enforce consistency between observed experience trajectories and hallucinated rollouts.\n",
      "///  \n",
      "/// Simple experiments demonstrate that the constraint slightly improves performance.\n",
      "/// \n",
      "\n",
      "Quality:\n",
      "\n",
      "While I think the idea of a consistency constraint is probably reasonable, I consider this a poorly executed exploration of the idea.\n",
      "///  The paper makes no serious effort to compare and contrast this idea with other efforts at model-based RL.\n",
      "///  The most glaring omission is comparison to very old ideas (such as dyna) and new ideas (such as imagination agents), both of which they cite.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Clarity:\n",
      "\n",
      "The paper is reasonably clear, although there are some holes.\n",
      "///  For example, in the experimental section, it is unclear what model-based RL algorithm is being used, and how it was modified to support the consistency constraint.\n",
      "///  \n",
      "/// (I did not read the appendix)\n",
      "/// .\n",
      "/// \n",
      "\n",
      "\n",
      "/// Originality:\n",
      "\n",
      "It is not clear how novel the central idea is.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Significance:\n",
      "\n",
      "This idea is not significant.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Pros:\n",
      "+ A simple, straightforward idea\n",
      "+ A good topic - progress in model-based RL is always welcome\n",
      "\n",
      "Cons:\n",
      "- Unclear how this is significantly different from other related work (such as imagination agents)\n",
      "- Experimental setup is poorly executed.\n",
      "/// \n",
      "  \n",
      "/// - Statistical significance of improvements is unclear\n",
      "  - No attempt to relate to any other method in the field\n",
      "  - No explanation of what algorithms are being used\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "Summary:\n",
      "\n",
      "This paper presents a simple auxiliary loss term for model-based RL that attempts to enforce consistency between observed experience trajectories and hallucinated rollouts.\n",
      ".\n",
      " \n",
      ".\n",
      "Simple experiments demonstrate that the constraint slightly improves performance.\n",
      ".\n",
      "\n",
      "\n",
      "Quality:\n",
      "\n",
      "While I think the idea of a consistency constraint is probably reasonable, I consider this a poorly executed exploration of the idea.\n",
      ".\n",
      " The paper makes no serious effort to compare and contrast this idea with other efforts at model-based RL.\n",
      ".\n",
      " The most glaring omission is comparison to very old ideas (such as dyna) and new ideas (such as imagination agents), both of which they cite.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Clarity:\n",
      "\n",
      "The paper is reasonably clear, although there are some holes.\n",
      ".\n",
      " For example, in the experimental section, it is unclear what model-based RL algorithm is being used, and how it was modified to support the consistency constraint.\n",
      ".\n",
      " \n",
      ".\n",
      "(I did not read the appendix)\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Originality:\n",
      "\n",
      "It is not clear how novel the central idea is.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Significance:\n",
      "\n",
      "This idea is not significant.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Pros:\n",
      "+ A simple, straightforward idea\n",
      "+ A good topic - progress in model-based RL is always welcome\n",
      "\n",
      "Cons:\n",
      "- Unclear how this is significantly different from other related work (such as imagination agents)\n",
      "- Experimental setup is poorly executed.\n",
      ".\n",
      "\n",
      "  \n",
      ".\n",
      "- Statistical significance of improvements is unclear\n",
      "  - No attempt to relate to any other method in the field\n",
      "  - No explanation of what algorithms are being used\n",
      "\n",
      "/// The work concerns convolution in the unit sphere.\n",
      "/// It differentiates itself from previously mentioned work by working in the volume space and not the surface space.\n",
      "/// While I can't say I understand all of the implications of the work\n",
      "/// ,  I was left with several questions.\n",
      "/// Many of these questions are in regards to claims made by the authors whose answer or reference was not made clear.\n",
      "/// \n",
      "\n",
      "\n",
      "/// - It was not made clear why there is a benefit to convolving the object in the unit sphere vs the unit cube, especially given that the work was not able to perform better than other work that was based on the unit sphere.\n",
      "/// This point was the stated problem of the paper.\n",
      "/// Although it was mentioned that the unit sphere preserves all of the points of the object, it isn't clear if the transformation causes any deformations of the object.\n",
      "/// Furthermore, the fixing of one axis seems to be a way to hack around problems of increased dimensionality, but there was no justification given.\n",
      "/// \n",
      "\n",
      "- How does the number of trainable layers help to differentiate resource usage.\n",
      "/// Wouldn't a better measure be number of parameters?\n",
      "/// The authors make that claim that shallowness is a virtue, but there is little discussion as to the size of each layer in comparable terms.\n",
      "/// \n",
      "\n",
      "- Why was no \"ablation\" or \"accuracy vs trained layers\" data shown for the Modelnet40 dataset?\n",
      "/// I would think that would be stronger evidence than for the Modelnet10 data.\n",
      "/// \n",
      "\n",
      "\n",
      "/// - Why wasn't the 1d conv net used for creating the viewing angles included in the size of the architecture? Was there a verification as to what the filters from this network were actually giving?\n",
      "999 [\"- Why wasn't the 1d conv net used for creating the viewing angles included in the size of the architecture\", 'Was there a verification as to what the filters from this network were actually giving', '']\n",
      "/// The authors mention how we should interpret them, but not enough information about the structure of the network is given to satisfy this question.\n",
      "/// \n",
      "\n",
      "\n",
      "/// - I would have liked to see a description of the types of features that are found by these networks.\n",
      "/// \n",
      "\n",
      "\n",
      "/// - The authors say they are only going to show experiments on one possible use case, but then make claims for other use cases.\n",
      "/// I am referencing that since the texture data in the datasets used is constant, there was no need to model the texture data.\n",
      "/// There is no experimental evidence to show this is the case, however.\n",
      "/// \n",
      "\n",
      "\n",
      "/// Overall, I think the paper would have been stronger if it had more experiments.\n",
      ".\n",
      "The work concerns convolution in the unit sphere.\n",
      ".\n",
      "It differentiates itself from previously mentioned work by working in the volume space and not the surface space.\n",
      ".\n",
      "While I can't say I understand all of the implications of the work\n",
      ".\n",
      ",  I was left with several questions.\n",
      ".\n",
      "Many of these questions are in regards to claims made by the authors whose answer or reference was not made clear.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "- It was not made clear why there is a benefit to convolving the object in the unit sphere vs the unit cube, especially given that the work was not able to perform better than other work that was based on the unit sphere.\n",
      ".\n",
      "This point was the stated problem of the paper.\n",
      ".\n",
      "Although it was mentioned that the unit sphere preserves all of the points of the object, it isn't clear if the transformation causes any deformations of the object.\n",
      ".\n",
      "Furthermore, the fixing of one axis seems to be a way to hack around problems of increased dimensionality, but there was no justification given.\n",
      ".\n",
      "\n",
      "\n",
      "- How does the number of trainable layers help to differentiate resource usage.\n",
      ".\n",
      "Wouldn't a better measure be number of parameters?\n",
      ".\n",
      "The authors make that claim that shallowness is a virtue, but there is little discussion as to the size of each layer in comparable terms.\n",
      ".\n",
      "\n",
      "\n",
      "- Why was no \"ablation\" or \"accuracy vs trained layers\" data shown for the Modelnet40 dataset?\n",
      ".\n",
      "I would think that would be stronger evidence than for the Modelnet10 data.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "- Why wasn't the 1d conv net used for creating the viewing angles included in the size of the architecture?\n",
      ".\n",
      "Was there a verification as to what the filters from this network were actually giving?\n",
      ".\n",
      "- Why wasn't the 1d conv net used for creating the viewing angles included in the size of the architecture? Was there a verification as to what the filters from this network were actually giving?\n",
      ".\n",
      "The authors mention how we should interpret them, but not enough information about the structure of the network is given to satisfy this question.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "- I would have liked to see a description of the types of features that are found by these networks.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "- The authors say they are only going to show experiments on one possible use case, but then make claims for other use cases.\n",
      ".\n",
      "I am referencing that since the texture data in the datasets used is constant, there was no need to model the texture data.\n",
      ".\n",
      "There is no experimental evidence to show this is the case, however.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Overall, I think the paper would have been stronger if it had more experiments.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_wrapper(text):\n",
    "    sentences = sci_nlp(text).sents\n",
    "    spans = [(s.start_char, s.end_char) for s in sentences]\n",
    "    final_spans = []\n",
    "    for start, end in spans:\n",
    "        print(\"///\", text[start:end])\n",
    "        span_text = text[start:end]\n",
    "        if '?' in span_text[:-1]:\n",
    "            inner_spans = question_tokenize(span_text)\n",
    "            final_spans += [(start + a, start + b) for a, b in inner_spans]\n",
    "        else:\n",
    "            final_spans.append((start, end))\n",
    "    return final_spans\n",
    "\n",
    "for pair in pair_obj[\"review_rebuttal_pairs\"][:10]:\n",
    "    text = pair[\"review_text\"][\"text\"]\n",
    "    tokenized = tokenize_wrapper(text)\n",
    "    for start, end in tokenized:\n",
    "        print(\".\")\n",
    "        print(text[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "victorian-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To my knowledge, this paper is probably the first one to apply few-shot learning concept into high-level computer vision tasks.\n",
      "\n",
      "In this paper's sense, segmentation.\n",
      "\n",
      "It proposes a general framework to few from the very few sample, extract a latent representation z, and apply it to do segmentation on a query.\n",
      "\n",
      "Cases of semantic, interactive and video segmentation are applied.\n",
      "\n",
      "Experiments are very thorough.\n",
      "\n",
      "\n",
      "\n",
      "We see too many variants of few-shot learning papers on mini-imagenet or omniglot.\n",
      "\n",
      "For the reason of applying to high-level segmentation, the paper already deserves an acceptance for the first work.\n",
      "\n",
      "I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = sci_nlp(\"\"\"\n",
    "To my knowledge, this paper is probably the first one to apply few-shot learning concept into high-level computer vision tasks. In this paper's sense, segmentation. It proposes a general framework to few from the very few sample, extract a latent representation z, and apply it to do segmentation on a query. Cases of semantic, interactive and video segmentation are applied. Experiments are very thorough.\n",
    "\n",
    "We see too many variants of few-shot learning papers on mini-imagenet or omniglot. For the reason of applying to high-level segmentation, the paper already deserves an acceptance for the first work. I believe this work would inspire many follow-ups in related domain (especially for high-level vision tasks)\n",
    "\"\"\").sents\n",
    "for i in d:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "equipped-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd?\n",
      "erere?\n",
      "rere?\n",
      "dfdfd.\n",
      "['abcd', 'erere', 'rere', 'dfdfd.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"abcd? erere? rere?dfdfd.\"\n",
    "parts = re.split(\"\\?\\s*\", sent)\n",
    "for part in parts[:-1]:\n",
    "    start = sent.find(part)\n",
    "    end = start + len(part) + 1\n",
    "    print(sent[start:end])\n",
    "print(sent[sent.find(parts[-1]):])\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-fiber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
