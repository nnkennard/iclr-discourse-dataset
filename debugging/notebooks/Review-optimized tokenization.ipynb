{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_nlp = English()\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "Comment = collections.namedtuple(\"Comment\", \"text sentences\")\n",
    "class Text(object):\n",
    "    def __init__(self, text):\n",
    "        sentence_texts = []\n",
    "        sentence_indices = []\n",
    "        for chunk in text.split(\"\\n\"):\n",
    "            doc = spacy_nlp(chunk)\n",
    "            for sent in doc.sents:\n",
    "                sentence_text = sent.text.strip()\n",
    "                if not sentence_text:\n",
    "                    continue\n",
    "                if sentence_indices:\n",
    "                    offset = sentence_indices[-1][1]\n",
    "                    index = offset + text[offset: ].find(sentence_text)\n",
    "                else :\n",
    "                    index = text.find(sentence_text)\n",
    "                sentence_texts.append(sentence_text)\n",
    "                sentence_indices.append((index, index + len(sentence_text)))\n",
    "        assert len(sentence_texts) == len(sentence_indices)\n",
    "\n",
    "        final_sentences = []\n",
    "        for i in range(len(sentence_texts) - 1):\n",
    "            start, end = sentence_indices[i]\n",
    "            sentence_text = sentence_texts[i]\n",
    "            next_sentence_start = sentence_indices[i + 1][0]\n",
    "            suffix = \"\\n\" * text[end: next_sentence_start].count(\"\\n\")\n",
    "            assert sentence_text == text[start: end]\n",
    "            final_sentences.append(make_sentence_dict(start, end, suffix))\n",
    "\n",
    "        if sentence_indices:\n",
    "            final_start, final_end = sentence_indices[-1]\n",
    "            final_sentences.append(make_sentence_dict(final_start, final_end, \"\"))\n",
    "\n",
    "        self.text = Comment(text, final_sentences)._asdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-experiment",
   "metadata": {},
   "source": [
    "Before tokenization, convert all references into \"Reference_n.\". Convert all citations into \"Citep_n.\" Remove any remaining et als.\n",
    "\n",
    "maybe also grab urls? do they get properly tokenized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openreview_lib import DATASETS\n",
    "for dataset in DATASETS:\n",
    "    PAIR_FILE = \"../../data/review_rebuttal_pair_dataset/\" + dataset + \".json\"\n",
    "    if dataset == 'unstructured':\n",
    "        continue\n",
    "    with open(PAIR_FILE, 'r') as f:\n",
    "        pair_obj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "DATE_MATCHER = re.compile(\"(\\d){4}\")\n",
    "REFERENCE_MATCHER = re.compile(\"^\\[..?\\]\")\n",
    "TITLE_CASE_WORD = re.compile(\"[A-Z][a-z]+\")\n",
    "END_DATE_MATCHER = re.compile(\".*[A-Z][a-z]+.*20(\\d){2}.?$\")\n",
    "START_SQB = re.compile(\"^\\[.*\")\n",
    "\n",
    "SHORT_PAREN_MATCHER = \"\\([^\\(\\)]\\)\"\n",
    "SQB_MATCHER = \"\\[.??\\]\"\n",
    "ETAL_MATCHER = \"et\\.?\\sal\\.?\"\n",
    "ARXIV = \".*arXiv.preprint.*\"\n",
    "ENDSWITH_DATE = re.compile(\".*(\\d){4}.?$\")\n",
    "\n",
    "placeholder_prefix = \"$$$ABCD\"\n",
    "placeholder_suffix = \"$$$\"\n",
    "\n",
    "def generate_placeholders():\n",
    "    i=0\n",
    "    while True:\n",
    "        new_placeholder = placeholder_prefix + str(i) + placeholder_suffix\n",
    "        i += 1\n",
    "        yield new_placeholder\n",
    "        \n",
    "def substitute(line, match, gen):\n",
    "    start, end = match.span()\n",
    "    before, match_text, after = line[:start], line[start:end], line[end:]\n",
    "    new_placeholder = next(gen)\n",
    "    updated_line = before + new_placeholder + after\n",
    "    return updated_line, {new_placeholder:match_text}\n",
    "\n",
    "for pair in pair_obj[\"review_rebuttal_pairs\"]:\n",
    "    review_text = pair[\"review_text\"][\"text\"]\n",
    "    \n",
    "    gen = generate_placeholders()\n",
    "    placeholders = collections.OrderedDict()\n",
    "    replaced_lines = \"\"\n",
    "    for line in review_text.split(\"\\n\"):\n",
    "        \n",
    "        updated_line = line\n",
    "        # Replace all occurrences of et al and misspellings\n",
    "#         etal_matches = list(re.finditer(ETAL_MATCHER, line))\n",
    "#         for match in reversed(sorted(etal_matches, key=lambda x:x.span()[0])):\n",
    "#             if re.match(\"\\s\", line[match.start() - 1]):\n",
    "#                 updated_line, mini_placeholder_map = substitute(updated_line, match, gen)\n",
    "#                 placeholders.update(mini_placeholder_map)\n",
    "\n",
    "        # Easier to search for references by searching for dates\n",
    "        flag = False\n",
    "        contains_date = re.findall(DATE_MATCHER, updated_line) # Really just checking for 4 digits\n",
    "        if contains_date:\n",
    "            start_sqb_match = re.match(START_SQB, updated_line)\n",
    "            if start_sqb_match is not None:\n",
    "                #updated_line, mini_placeholder_map = substitute(updated_line, start_sqb_match, gen)\n",
    "                #placeholders.update(mini_placeholder_map)\n",
    "                print(\"1\\t\"+line)\n",
    "                flag = True\n",
    "            else:\n",
    "                maybe_arxiv_match = re.match(ARXIV, updated_line)\n",
    "                if maybe_arxiv_match is not None:\n",
    "#                     updated_line, mini_placeholder_map = substitute(updated_line, maybe_arxiv_match, gen)\n",
    "#                     placeholders.update(mini_placeholder_map)\n",
    "                    print(\"1\\t\"+line)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    namelike = re.findall(TITLE_CASE_WORD, updated_line)\n",
    "                    if len(namelike) > 3 and len(updated_line) < 200 and re.match(\n",
    "                    ENDSWITH_DATE, updated_line):\n",
    "#                         updated_line, mini_placeholder_map = substitute(\n",
    "#                             updated_line, \n",
    "#                             re.match(ENDSWITH_DATE, updated_line), gen)\n",
    "#                 placeholders.update(mini_placeholder_map)\n",
    "                \n",
    "                        print(\"1\\t\"+line)\n",
    "                flag = True\n",
    "                \n",
    "        if not flag:\n",
    "            print(\"0\\t\"+line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "k =spacy_nlp('The paper presents extensive, interesting results. I do want to point that they seem to be considerably off of the LibriSpeech state of the art, e.g. see K. Irie $$$ABCD1$$$ Interspeech 2019.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in k.sents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = re.findall(ETAL_MATCHER, \"Why not a direct comparison with the models proposed by Wang et al., 2019, He et al., 2019?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"et al\" == \"et al\")\n",
    "\n",
    "for i in l:\n",
    "    print(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-search",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "thorough-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\n",
      "In the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\n",
      "Without applications in other domains, it is not immediately clear what the paper's implication is.\n",
      "\n",
      "\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\n",
      "Can authors provide more details, e.g., empirical results, about it.\n",
      "What is its rationale.\n",
      "\n",
      "\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions.\n",
      "I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating.\n",
      "\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right.\n",
      "I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\n",
      "\n",
      "\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (2018).\n",
      "\n",
      "\n",
      "\n",
      "- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "To empirically resolve the above concerns\n",
      ", it is necessary to present the empirical comparison with the \"static\" softmax.\n",
      "\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\n",
      "\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\n",
      "\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\n",
      "\n",
      "\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\n",
      "It, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\n",
      "The distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\n",
      "To validate the proposed method, it is required to compare the method with such a different types of loss function.\n",
      "\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\n",
      "Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\n"
     ]
    }
   ],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "sci_nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "\n",
    "doc = sci_nlp(difficult_passage.replace(\"?\", \".\"))\n",
    "\n",
    "for s in doc.sents:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hungarian-thriller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\n",
      "In the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\n",
      "Without applications in other domains, it is not immediately clear what the paper's implication is.\n",
      "\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\n",
      "Can authors provide more details, e.g., empirical results, about it?\n",
      "What is its rationale?\n",
      "\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\n",
      "I would believe that, since there are rarely questions in scientific text.\n",
      "But wow, isn't that so irritating?\n",
      "\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\n",
      "I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\n",
      "\n",
      "\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\n",
      "2018).\n",
      "\n",
      "\n",
      "- Ablation study (This has been described in detail in Tab.\n",
      "1 and also in Sec.\n",
      "3)\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\n",
      "\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\n",
      "\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\n",
      "2017].\n",
      "\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\n",
      "\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\n",
      "It, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\n",
      "The distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\n",
      "To validate the proposed method, it is required to compare the method with such a different types of loss function.\n",
      "\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\n",
      "Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\n",
      "9117–9126.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy.lang.en import English\n",
    "spacy_nlp = English()\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "\n",
    "doc = spacy_nlp(difficult_passage)\n",
    "\n",
    "for s in doc.sents:\n",
    "    print(s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox\n",
    "\n",
    "current_rebuttal = \"\"\"Thank you for your review and we would be delighted to address your concerns, but do require some clarifications.\n",
    "\n",
    "While a learnable lambda could be considered we would argue that the learning of this parameter beyond the grid-search applied in the submission is somewhat tangential to our primary contribution: a unified framework which lends itself to targeted representation analysis and modification.\n",
    "\n",
    "1)\n",
    "The notion of a map of \\lambdas sounds interesting. However, at present, it is not clear to us what this refers to as \\lambda is a weighting on a loss term. Clarification would be much appreciated so we can fully engage with this point. As far as the existing approach is concerned, Figure 6 illustrates the influence of \\lambda on the accuracy and correlation of global and local stability prediction.\n",
    "\n",
    "2)\n",
    "The inconsistent correlations between the two tasks are exactly the scenarios where stethoscopes come into their own: testing positive and negative regimes of lambda (corresponding to auxiliary and adversarial training, respectively) reveals the interplay between the two tasks and potentially allows for de-biasing the algorithm as shown in Figure 6a. Therefore, in contrast to the design not considering these relationships, it explicitly addresses them.\n",
    "\n",
    "Could you please elaborate on the comment ’the current design […] simply sums them up’? The stethoscope module has its own trainable parameters and a separate loss function. Only the encoder shares weights between main and secondary task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "doc = spacy_nlp(current_rebuttal)\n",
    "\n",
    "for s in doc.sents:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = spacy_nlp(current_rebuttal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "independent-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_passage = \"\"\"However, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.\n",
    "\n",
    "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\". Can authors provide more details, e.g., empirical results, about it? What is its rationale?\n",
    "\n",
    "I wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
    "\n",
    "In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. \n",
    "\n",
    "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
    "\n",
    "generative models like in Song et al. (2018). \n",
    "\n",
    "- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
    "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\n",
    "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\n",
    "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\n",
    "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\n",
    "\n",
    "- Other loss function\n",
    "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization. It, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories. The distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work. To validate the proposed method, it is required to compare the method with such a different types of loss function.\n",
    "\n",
    "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018). Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "temporal-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy                                                                                                                                                                                                                                       Scispacy\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m                               \u001b[31mHowever, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all.\u001b[0m\n",
      "\u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m                                                                                                                                                 \u001b[34mIn the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied.\u001b[0m\n",
      "\u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m                                                                                                                                         \u001b[31mWithout applications in other domains, it is not immediately clear what the paper's implication is.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m                                                                                   \u001b[0m\n",
      "\u001b[31mCan authors provide more details, e.g., empirical results, about it?\u001b[0m                                                                                                                                                                        \u001b[31ma) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\".\u001b[0m\n",
      "\u001b[34mWhat is its rationale?\u001b[0m                                                                                                                                                                                                                      \u001b[34mCan authors provide more details, e.g., empirical results, about it?\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mWhat is its rationale?\u001b[0m\n",
      "\n",
      "I wonder if this tokenizer is just not able to tokenize on questions?\u001b[0m\n",
      "\u001b[34mI would believe that, since there are rarely questions in scientific text.\u001b[0m                                                                                                                                                                  \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[31mBut wow, isn't that so irritating?\u001b[0m                                                                                                                                                                                                          \u001b[31mI wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "                                                                                                                                                                                                                                            In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right?\u001b[0m                                                                                                                                                               b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "                                                                                                                                                                                                                                            generative models like in Song et al. (2018).\u001b[0m\n",
      "\u001b[31mI agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\u001b[0m                \u001b[31m\n",
      "\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
      "                                                                                                                                                                                                                                            To empirically resolve the above concerns\u001b[0m\n",
      "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "\n",
      "generative models like in Song et al. (\u001b[0m\n",
      "\u001b[31m2018).\u001b[0m                                                                                                                                                                                                                                      \u001b[31m, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m\n",
      "                                                                                                                                                                                                                                            Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "- Ablation study (This has been described in detail in Tab.\u001b[0m\n",
      "\u001b[31m1 and also in Sec.\u001b[0m                                                                                                                                                                                                                          \u001b[31m\n",
      "                                                                                                                                                                                                                                            And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\u001b[0m\n",
      "\u001b[34m3)                                                                                                                                                                                                                                          \u001b[34m\n",
      "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\u001b[0m                                                                                                                   In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31m\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\u001b[0m\n",
      "                                                                                                                                                                                                                                            \u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34m- Other loss function\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al.,\u001b[0m                                                                      For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[31m2017].\u001b[0m                                                                                                                                                                                                                                      \u001b[31mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m\n",
      "\u001b[34m                                                                                                                                                                                                                                            \u001b[34mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m\n",
      "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\u001b[0m\n",
      "\u001b[31m                                                                                                                                                                                                                                            \u001b[31mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\n",
      "- Other loss function\n",
      "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization.\u001b[0m\n",
      "\u001b[34mIt, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories.\u001b[0m                              \u001b[34m\n",
      "\n",
      "                                                                                                                                                                                                                                            [a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[31mThe distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work.\u001b[0m                                                                                                                  \u001b[31mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\u001b[0m\n",
      "\u001b[34mTo validate the proposed method, it is required to compare the method with such a different types of loss function.\u001b[0m\n",
      "\u001b[31m\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018).\u001b[0m\n",
      "\u001b[34mRethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp.\u001b[0m\n",
      "\u001b[31m9117–9126.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from termcolor import colored\n",
    "import tabulate\n",
    "\n",
    "sci_nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "spacy_nlp = English()\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "\n",
    "def tokenize_scispacy(text):\n",
    "    return list(sci_nlp(text).sents)\n",
    "def tokenize_spacy(text):\n",
    "    return list(spacy_nlp(text).sents)\n",
    "\n",
    "\n",
    "COLORS = [\"red\", \"blue\"]\n",
    "def build_column(tokenized_sentences):\n",
    "    column = []\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        color = COLORS[i%2]\n",
    "        column += [colored(sentence, color)]\n",
    "    return column\n",
    "\n",
    "def build_tokenized_col(tokenize_fn, text):\n",
    "    tokenized = tokenize_fn(text)\n",
    "    return build_column(tokenized)\n",
    "\n",
    "def make_cols(text):\n",
    "    spacy_col = build_tokenized_col(tokenize_spacy, text)\n",
    "    scispacy_col = build_tokenized_col(tokenize_scispacy, text)\n",
    "    max_len = max([len(spacy_col), len(scispacy_col)])\n",
    "\n",
    "    spacy_col += [\"\"] * (max_len - len(spacy_col))\n",
    "    scispacy_col += [\"\"] * (max_len - len(scispacy_col))\n",
    "    return spacy_col, scispacy_col\n",
    "\n",
    "\n",
    "def make_table(text):\n",
    "    return tabulate.tabulate(\n",
    "        zip(*make_cols(text)),\n",
    "        headers = [\"Spacy\", \"Scispacy\"])\n",
    "\n",
    "for chunk in difficult_passage.split(\"\\n\"):\n",
    "    if not chunk:\n",
    "        continue\n",
    "    print(make_table(difficult_passage))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a year, use scispacy\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "earlier-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_thing_with_questions = \"Do you have a question? No, I have a comment.\"\n",
    "another_thing_with_questions = \"\"\"The introduction and the title does not match. Metric learning does not require to specify the dimension; while the embedding has to specify the reduced dimension. I feel confused that the authors mix these two concepts.\n",
    "\n",
    "The objective in (1) is very close to that of t-SNE[5], where it uses the KL as the objective. Then other update formula are similar.  \n",
    "\n",
    "This paper facilitates the effect of temperature in the Softmax function to heuristically learn a compact and spread-out embedding. However, such an idea have been widely used and investigated in Reinforcement learning [1], Knowledge distillation [2], classification [3] and discrete variable optimization [4] and t-SNE visualization [5] etc. Thus, the insight about the temperature effect on the embedding from the second last layer, cannot be novel any more. Based on this, the proposed ``heating-up” strategy to leverage its effect on the embedding is heuristic, since the temperature parameter is manually set instead of automatically learning. In this case, I do expect the authors should provide more in-depth theoretical analysis. \n",
    "\n",
    "The authors do not present more experimental results on the correlation between the final performance and this temperature setting. \n",
    "\n",
    "Besides, as the alpha increases or decreases, the side-effect on the learning rate setting for the optimization have not clearly analyzed, which leaves more concerns on tuning performance. \n",
    "\n",
    "\n",
    "[1] Sutton, R. S. and Barto A. G. Reinforcement Learning: An Introduction. The MIT Press, Cambridge, MA, 1998.\n",
    "[2] Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. NIPS 2015.\n",
    "[3] Guo, Chuan, et al. \"On calibration of modern neural networks.\" ICML 2017.\n",
    "[4] Jang E, Gu S, Poole B. Categorical reparameterization with gumbel-softmax. ICLR 2017.\n",
    "[5] Maaten L, Hinton G. Visualizing data using t-SNE[J]. Journal of machine learning research, 2008, 9(Nov): 2579-2605.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial-taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction and the title does not match.\n",
      "Metric learning does not require to specify the dimension; while the embedding has to specify the reduced dimension.\n",
      "I feel confused that the authors mix these two concepts.\n",
      "\n",
      "\n",
      "The objective in (1) is very close to that of t-SNE[5], where it uses the KL as the objective.\n",
      "Then other update formula are similar.\n",
      " \n",
      "\n",
      "This paper facilitates the effect of temperature in the Softmax function to heuristically learn a compact and spread-out embedding.\n",
      "However, such an idea have been widely used and investigated in Reinforcement learning [1], Knowledge distillation [2], classification [3] and discrete variable optimization [4] and t-SNE visualization [5] etc.\n",
      "Thus, the insight about the temperature effect on the embedding from the second last layer, cannot be novel any more.\n",
      "Based on this, the proposed ``heating-up” strategy to leverage its effect on the embedding is heuristic, since the temperature parameter is manually set instead of automatically learning.\n",
      "In this case, I do expect the authors should provide more in-depth theoretical analysis.\n",
      "\n",
      "\n",
      "The authors do not present more experimental results on the correlation between the final performance and this temperature setting.\n",
      "\n",
      "\n",
      "\n",
      "Besides, as the alpha increases or decreases, the side-effect on the learning rate setting for the optimization have not clearly analyzed, which leaves more concerns on tuning performance.\n",
      "\n",
      "\n",
      "\n",
      "[1] Sutton, R. S. and Barto A. G. Reinforcement Learning: An Introduction. The MIT Press, Cambridge, MA, 1998.\n",
      "[2] Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. NIPS 2015.\n",
      "\n",
      "[3] Guo, Chuan, et al. \"On calibration of modern neural networks.\" ICML 2017.\n",
      "[4] Jang E, Gu S, Poole B. Categorical reparameterization with gumbel-softmax. ICLR 2017.\n",
      "\n",
      "[5] Maaten L, Hinton G. Visualizing data using t-SNE[J]. Journal of machine learning research, 2008, 9(Nov): 2579-2605.\n"
     ]
    }
   ],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from termcolor import colored\n",
    "import tabulate\n",
    "\n",
    "sci_nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "spacy_nlp = English()\n",
    "spacy_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "#doc = spacy_nlp(another_thing_with_questions.replace(\"?\",\".\"))\n",
    "#doc = spacy_nlp(another_thing_with_questions)\n",
    "doc = sci_nlp(another_thing_with_questions)\n",
    "\n",
    "for s in doc.sents:\n",
    "#     print(type(s))\n",
    "#     print(dir(s))\n",
    "#     print(s.start, s.end)\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "classical-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"However, my concern is that the paper focuses only on a very specific application domain, and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of \"Neural Stethoscopes\" could be much more generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.\n",
    "\n",
    "a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\". Can authors provide more details, e.g., empirical results, about it? What is its rationale?\n",
    "\n",
    "I wonder if this tokenizer is just not able to tokenize on questions? I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
    "\n",
    "In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. \n",
    "\n",
    "b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
    "\n",
    "generative models like in Song et al. (2018). \n",
    "\n",
    "- Ablation study (This has been described in detail in Tab. 1 and also in Sec. 3)\n",
    "To empirically resolve the above concerns, it is necessary to present the empirical comparison with the \"static\" softmax.\n",
    "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\n",
    "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\n",
    "In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.\n",
    "\n",
    "- Other loss function\n",
    "For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization. It, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories. The distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work. To validate the proposed method, it is required to compare the method with such a different types of loss function.\n",
    "\n",
    "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018). Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "quick-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_span_map(nlp_pipeline, text):\n",
    "    span_map = collections.OrderedDict()\n",
    "    doc = nlp_pipeline(text.replace(\"\\n\", \" \"))\n",
    "    return [(s.start_char, s.end_char) for s in doc.sents]\n",
    "\n",
    "normal_span_list = get_span_map(spacy_nlp, text)\n",
    "sci_span_list = get_span_map(sci_nlp, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "private-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 10, 16: 18, 24: 26}\n",
      "I would believe that, since there are rarely questions in scientific text. But wow, isn't that so irritating?\n",
      "\n",
      "In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training.\n",
      "********************************************************************************\n",
      "Namely, the methods of HLN/HBN should be carefully compared to LN/BN of \"alpha=4\", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.\n",
      "And, it would be better to show the performance of heated-up softmax \"without\" normalization to show the important role of the normalization, as done in [Wang et al., 2017].\n",
      "********************************************************************************\n",
      "\n",
      "[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018). Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "final_spans = collections.OrderedDict()\n",
    "\n",
    "def get_last_index(span_list):\n",
    "    return span_list[-1][1]\n",
    "\n",
    "def get_contained_spans(super_span, span_list):\n",
    "    super_start, super_end = super_span\n",
    "    contained_spans = []\n",
    "    for i, (start, end) in enumerate(span_list):\n",
    "        if start >= super_start and end <= super_end:\n",
    "            contained_spans.append(i)\n",
    "    if not contained_spans:\n",
    "        return None, None\n",
    "    return contained_spans[0], contained_spans[-1]\n",
    "        \n",
    "# Finding a sci span that contains a bunch of normal spans. we need to merge the normal spans\n",
    "normal_spans_to_merge = {}\n",
    "for sci_span in sci_span_list:\n",
    "    sci_start, sci_end = sci_span\n",
    "    first_contained_span, last_contained_span = get_contained_spans(sci_span, normal_span_list)\n",
    "    if first_contained_span is None:\n",
    "        continue\n",
    "    if last_contained_span - first_contained_span > 1:\n",
    "        normal_spans_to_merge[first_contained_span] = last_contained_span\n",
    "        \n",
    "print(normal_spans_to_merge)\n",
    "\n",
    "for start_span, end_span in normal_spans_to_merge.items():\n",
    "    print(text[normal_span_list[start_span][0]:normal_span_list[end_span][1]])\n",
    "    print(\"*\" * 80)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If sci span has split it, we are fine with it being split (except parentheses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
